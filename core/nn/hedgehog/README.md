# Softmax Mimicry

paper: https://arxiv.org/pdf/2402.04347

<img src="attachments/hedgehog_1.png"></img>

This exploits the fact that the only difference between linear attention and standard is
the softmax is replaced with function mappings.

<img src="attachments/hedgehog_2.png"></img>
<img src="attachments/hedgehog_3.png"></img>

# Low rank Linear Conversion with Attention Transfer
<img src="attachments/hedgehog_4.png"></img>
paper https://arxiv.org/pdf/2410.10254
