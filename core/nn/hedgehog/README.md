# Softmax Mimicry

paper: https://arxiv.org/pdf/2402.04347

<img src="attachments/hedgehog_1.png"></img>

This exploits the fact that the only difference between linear attention and standard is
the softmax is replaced with function mappings.

<img src="attachments/hedgehog_2.png"></img>
